#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Filstar checker ‚Äî —Å–µ—Ä–∏–π–Ω–æ –∏ —â–∞–¥—è—â–æ, —Å –ø–æ-—à–∏—Ä–æ–∫–∞ –ª–æ–≥–∏–∫–∞ –∑–∞ —Ç—ä—Ä—Å–µ–Ω–µ –Ω–∞ –ø—Ä–æ–¥—É–∫—Ç–æ–≤–∏ –ª–∏–Ω–∫–æ–≤–µ.
–ù–∞ –≤—Å—è–∫–æ –ø—É—Å–∫–∞–Ω–µ –æ–±—Ä–∞–±–æ—Ç–≤–∞ –≤—Å–∏—á–∫–∏ SKU –æ—Ç CSV –∏ –ø—Ä–µ–∑–∞–ø–∏—Å–≤–∞ results/not_found.

- –ó–∞ –≤—Å–µ–∫–∏ SKU:
    1) GET /search?term=<sku> (bg/en —Å—ä—â–æ)
    2) –í–∞–¥–∏ –∫–∞–Ω–¥–∏–¥–∞—Ç-–ª–∏–Ω–∫–æ–≤–µ —Å –Ω—è–∫–æ–ª–∫–æ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞ + regex fallback
    3) –ó–∞ –≤—Å–µ–∫–∏ –∫–∞–Ω–¥–∏–¥–∞—Ç: –ø–∞—Ä—Å–≤–∞ —Ç–∞–±–ª–∏—Ü–∞—Ç–∞ #fast-order-table, –Ω–∞–º–∏—Ä–∞ —Ä–µ–¥ –ø–æ "–ö–û–î"
    4) –í–∑–∏–º–∞ –Ω–æ—Ä–º–∞–ª–Ω–∞ —Ü–µ–Ω–∞ (–ª–≤.) –∏ –±—Ä–æ–π–∫–∞ (–æ—Ç counter-box input[type=text])

- Debug:
    - –ó–∞–ø–∏—Å–≤–∞ search HTML, –∞–∫–æ –Ω—è–º–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–∏
    - –ó–∞–ø–∏—Å–≤–∞ product HTML, –∞–∫–æ –Ω–µ –Ω–∞–º–µ—Ä–∏ —Ä–µ–¥/—Ü–µ–Ω–∞

–í–ù–ò–ú–ê–ù–ò–ï: –±–µ–∑ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ—Å—Ç/–Ω–∏—à–∫–∏. –ò–º–∞ –ø–∞—É–∑–∏ –º–µ–∂–¥—É –∑–∞—è–≤–∫–∏—Ç–µ.
"""

import csv
import os
import re
import sys
import time
from typing import List, Tuple, Optional
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup

# ---------------- –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è ----------------
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
SKU_CSV = os.path.join(BASE_DIR, "sku_list_filstar.csv")

RES_CSV = os.path.join(BASE_DIR, "results_filstar.csv")
NF_CSV  = os.path.join(BASE_DIR, "not_found_filstar.csv")

DEBUG_DIR = os.path.join(BASE_DIR, "debug_html")
os.makedirs(DEBUG_DIR, exist_ok=True)

SEARCH_URLS = [
    "https://filstar.com/search?term={q}",
    "https://filstar.com/bg/search?term={q}",
    "https://filstar.com/en/search?term={q}",
]

# –©–∞–¥—è—â–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
REQUEST_DELAY = 0.45      # —Å–µ–∫. –ø–∞—É–∑–∞ –º–µ–∂–¥—É HTTP –∑–∞—è–≤–∫–∏
DELAY_BETWEEN_SKUS = 0.9  # —Å–µ–∫. –ø–∞—É–∑–∞ –º–µ–∂–¥—É SKU
TIMEOUT = 20              # —Ç–∞–π–º–∞—É—Ç
RETRIES = 3               # —Ä–µ—Ç—Ä–∞–∏–∏
MAX_CANDIDATES = 20       # –º–∞–∫—Å–∏–º—É–º –∫–∞–Ω–¥–∏–¥–∞—Ç –ª–∏–Ω–∫–æ–≤–µ

HEADERS = {
    "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
                  "(KHTML, like Gecko) Chrome/127 Safari/537.36",
    "Accept-Language": "bg-BG,bg;q=0.9,en-US;q=0.8,en;q=0.7",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Connection": "keep-alive",
}

# ---------------- –ü–æ–º–æ—â–Ω–∏ ----------------
def norm(s: str) -> str:
    return (s or "").strip()

def only_digits(s: str) -> str:
    return re.sub(r"\D+", "", s or "")

def save_debug_html_text(text: str, sku: str, tag: str):
    try:
        path = os.path.join(DEBUG_DIR, f"debug_{sku}_{tag}.html")
        with open(path, "w", encoding="utf-8") as f:
            f.write(text)
        print(f"   üêû Debug HTML –∑–∞–ø–∏—Å–∞–Ω: {path}")
    except Exception:
        pass

# ---------------- HTTP –∫–ª–∏–µ–Ω—Ç (—Å–µ—Ä–∏–π–Ω–æ, —Å –ø–∞—É–∑–∞) ----------------
class Http:
    def __init__(self):
        self.s = requests.Session()
        self.s.headers.update(HEADERS)
        self._last_ts = 0.0

    def get(self, url: str) -> requests.Response:
        elapsed = time.time() - self._last_ts
        if elapsed < REQUEST_DELAY:
            time.sleep(REQUEST_DELAY - elapsed)
        last_exc = None
        for attempt in range(1, RETRIES + 1):
            try:
                r = self.s.get(url, timeout=TIMEOUT, allow_redirects=True)
                self._last_ts = time.time()
                return r
            except requests.RequestException as e:
                last_exc = e
                time.sleep(0.6 * attempt)
        raise last_exc or RuntimeError("HTTP GET failed")

HTTP = Http()

# ---------------- I/O ----------------
def read_skus(path: str) -> List[str]:
    skus = []
    with open(path, newline="", encoding="utf-8") as f:
        r = csv.reader(f)
        _ = next(r, None)  # —Ö–µ–¥—ä—Ä
        for row in r:
            if not row: continue
            v = norm(row[0])
            if v and v.lower() != "sku":
                skus.append(v)
    return skus

def init_result_files():
    # –í–ò–ù–ê–ì–ò –ø—Ä–µ–∑–∞–ø–∏—Å–≤–∞–º–µ –∑–∞–≥–ª–∞–≤–∫–∏—Ç–µ (–Ω–æ–≤ run = –Ω–æ–≤–∏ —Ä–µ–∑—É–ª—Ç–∞—Ç–∏)
    with open(RES_CSV, "w", newline="", encoding="utf-8") as f:
        csv.writer(f).writerow(["SKU", "–ù–∞–ª–∏—á–Ω–æ—Å—Ç", "–ë—Ä–æ–π–∫–∏", "–¶–µ–Ω–∞ (–Ω–æ—Ä–º–∞–ª–Ω–∞ –ª–≤.)"])
    with open(NF_CSV, "w", newline="", encoding="utf-8") as f:
        csv.writer(f).writerow(["SKU"])

def append_result(row):
    with open(RES_CSV, "a", newline="", encoding="utf-8") as f:
        csv.writer(f).writerow(row)

def append_nf(sku: str):
    with open(NF_CSV, "a", newline="", encoding="utf-8") as f:
        csv.writer(f).writerow([sku])

# ---------------- –¢—ä—Ä—Å–µ–Ω–µ: –∫–∞–Ω–¥–∏–¥–∞—Ç–∏ ----------------
def parse_search_candidates(html: str) -> List[str]:
    """–ò–∑–≤–ª–∏—á–∞ –ø—Ä–æ–¥—É–∫—Ç–æ–≤–∏ –ª–∏–Ω–∫–æ–≤–µ –æ—Ç search HTML —Å –Ω—è–∫–æ–ª–∫–æ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞ + regex fallback."""
    soup = BeautifulSoup(html, "lxml")
    out = []

    # 1) –ö–ª–∞—Å–∏—á–µ—Å–∫–∏—è—Ç: .product-item-wapper a.product-name
    for a in soup.select(".product-item-wapper a.product-name"):
        href = (a.get("href") or "").strip()
        if href:
            if href.startswith("/"):
                href = urljoin("https://filstar.com", href)
            out.append(href)

    # 2) –ü–æ–Ω—è–∫–æ–≥–∞ –∑–∞–≥–ª–∞–≤–∏—è—Ç–∞ —Å–∞ –≤ .product-title a
    for a in soup.select(".product-title a"):
        href = (a.get("href") or "").strip()
        if href:
            if href.startswith("/"):
                href = urljoin("https://filstar.com", href)
            out.append(href)

    # 3) Fallback: regex –∑–∞ <a href="/Some-Product-1234">
    if not out:
        for m in re.finditer(r'href="(/[^"]*?-?\d+)"', html):
            href = m.group(1)
            if "/search" in href or "term=" in href:
                continue
            href_full = urljoin("https://filstar.com", href)
            out.append(href_full)

    # –ø—Ä–µ–º–∞—Ö–≤–∞–Ω–µ –Ω–∞ –¥—É–±–ª–∏ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ
    seen, uniq = set(), []
    for h in out:
        if h not in seen:
            seen.add(h)
            uniq.append(h)
    return uniq[:MAX_CANDIDATES]

# ---------------- –ü—Ä–æ–¥—É–∫—Ç–æ–≤–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞: —Ä–µ–¥ –ø–æ –ö–û–î ----------------
def extract_row_data_from_product_html(html: str, sku: str) -> Tuple[Optional[str], Optional[int], Optional[str]]:
    """
    –í—Ä—ä—â–∞ (status, qty, normal_price_lv) –∑–∞ —Ç–æ—á–Ω–∏—è —Ä–µ–¥ –ø–æ '–ö–û–î'.
    - –ù–æ—Ä–º–∞–ª–Ω–∞ —Ü–µ–Ω–∞: –æ—Ç <strike> –∞–∫–æ –∏–º–∞ –Ω–∞–º–∞–ª–µ–Ω–∏–µ, –∏–Ω–∞—á–µ –ø—ä—Ä–≤–∞—Ç–∞ '... –ª–≤.' –≤ —Ü–µ–Ω–æ–≤–∞—Ç–∞ –∫–ª–µ—Ç–∫–∞/—Ä–µ–¥–∞.
    - –ë—Ä–æ–π–∫–∞: –æ—Ç counter-box input[type=text] value (–≤–∏–¥–∏–º–∞ –∏ –±–µ–∑ –ª–æ–≥–∏–Ω).
    """
    soup = BeautifulSoup(html, "lxml")
    tbody = soup.select_one("#fast-order-table tbody")
    if not tbody:
        return None, None, None

    rows = tbody.select("tr")
    target_row = None

    # A) –û–ø–∏—Ç–∞–π —Å td.td-sky (–∫—ä–¥–µ—Ç–æ –µ "–ö–û–î")
    for row in rows:
        code_td = row.select_one("td.td-sky")
        code_digits = only_digits(code_td.get_text(" ", strip=True)) if code_td else ""
        if code_digits == str(sku):
            target_row = row
            break

    # B) –ê–∫–æ –Ω–µ —Å—Ä–∞–±–æ—Ç–∏, –æ–±—Ö–æ–∂–¥–∞–º–µ –≤—Å–∏—á–∫–∏ td –∏ —Ç—ä—Ä—Å–∏–º —á–∏—Å—Ç–æ —Å—ä–≤–ø–∞–¥–µ–Ω–∏–µ
    if target_row is None:
        for row in rows:
            tds = row.find_all("td")
            for td in tds:
                if only_digits(td.get_text(" ", strip=True)) == str(sku):
                    target_row = row
                    break
            if target_row is not None:
                break

    if target_row is None:
        # C) –ü–æ—Å–ª–µ–¥–µ–Ω —à–∞–Ω—Å: —Ç—ä—Ä—Å–µ–Ω–µ –ø–æ —Ç–µ–∫—Å—Ç –≤ —Ä–µ–¥–∞
        for row in rows:
            txt = row.get_text(" ", strip=True)
            if re.search(rf"\b{re.escape(str(sku))}\b", txt):
                target_row = row
                break

    if target_row is None:
        return None, None, None

    # --- –¶–µ–Ω–∞ (–Ω–æ—Ä–º–∞–ª–Ω–∞) ---
    price = None
    strike = target_row.find("strike")
    if strike:
        m = re.search(r"(\d+[.,]?\d*)\s*–ª–≤", strike.get_text(" ", strip=True), re.I)
        if m:
            price = m.group(1).replace(",", ".")
    if not price:
        price_td = None
        for td in target_row.find_all("td"):
            if td.find(string=lambda t: isinstance(t, str) and "–¶–ï–ù–ê –ù–ê –î–†–ï–ë–ù–û" in t):
                price_td = td
                break
        txt = price_td.get_text(" ", strip=True) if price_td else target_row.get_text(" ", strip=True)
        m2 = re.search(r"(\d+[.,]?\d*)\s*–ª–≤", txt, re.I)
        if m2:
            price = m2.group(1).replace(",", ".")

    # --- –ù–∞–ª–∏—á–Ω–æ—Å—Ç/–±—Ä–æ–π–∫–∞ (counter-box input[type=text]) ---
    qty = 0
    status = "Unknown"
    inp = target_row.select_one(".counter-box input[type='text']")
    if inp:
        val = (inp.get("value") or "").strip()
        if val.isdigit():
            qty = int(val)
            status = "–ù–∞–ª–∏—á–µ–Ω" if qty > 0 else "–ò–∑—á–µ—Ä–ø–∞–Ω"

    return status, qty, price

# ---------------- –õ–æ–≥–∏–∫–∞ –∑–∞ 1 SKU (—Å–µ—Ä–∏–π–Ω–æ) ----------------
def process_one_sku(sku: str):
    q = only_digits(sku) or sku
    print(f"\n‚û°Ô∏è –û–±—Ä–∞–±–æ—Ç–≤–∞–º SKU: {q}")

    candidates = []
    search_html_saved = False

    # 1) —Ç—ä—Ä—Å–µ–Ω–µ
    for su in SEARCH_URLS:
        url = su.format(q=q)
        try:
            r = HTTP.get(url)
            if r.status_code == 200 and r.text:
                c = parse_search_candidates(r.text)
                if c:
                    candidates.extend(c)
                else:
                    # –∑–∞–ø–∞–∑–∏ search HTML —Å–∞–º–æ –≤–µ–¥–Ω—ä–∂ –∑–∞ –¥–µ–±—ä–≥
                    if not search_html_saved:
                        save_debug_html_text(r.text, q, "search_no_candidates")
                        search_html_saved = True
        except Exception:
            pass
        if len(candidates) >= MAX_CANDIDATES:
            break

    # —É–Ω–∏–∫–∞–ª–∏–∑–∏—Ä–∞–π
    seen, uniq = set(), []
    for h in candidates:
        if h not in seen:
            seen.add(h)
            uniq.append(h)
    candidates = uniq[:MAX_CANDIDATES]

    if not candidates:
        print(f"‚ùå –ù—è–º–∞ —Ä–µ–∑—É–ª—Ç–∞—Ç–∏ –≤ —Ç—ä—Ä—Å–∞—á–∫–∞—Ç–∞ –∑–∞ {q}")
        append_nf(q)
        return

    # 2) –æ–±—Ö–æ–∂–¥–∞–º–µ –∫–∞–Ω–¥–∏–¥–∞—Ç –ø—Ä–æ–¥—É–∫—Ç–∏—Ç–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–Ω–æ
    saved_product_debug = False
    for link in candidates:
        try:
            r = HTTP.get(link)
            if r.status_code != 200 or not r.text:
                continue
            status, qty, price = extract_row_data_from_product_html(r.text, q)
            if price is not None:
                print(f"  ‚úÖ {q} ‚Üí {price} –ª–≤. | {status} ({qty} –±—Ä.) | {link}")
                append_result([q, status or "Unknown", qty or 0, price])
                return
            else:
                if not saved_product_debug:
                    save_debug_html_text(r.text, q, "no_price_or_row")
                    saved_product_debug = True
        except Exception:
            continue

    print(f"‚ùå –ù–µ –Ω–∞–º–µ—Ä–∏—Ö SKU {q} –≤ {len(candidates)} —Ä–µ–∑—É–ª—Ç–∞—Ç–∞.")
    append_nf(q)

# ---------------- main (—Å–µ—Ä–∏–π–Ω–æ) ----------------
def main():
    if not os.path.exists(SKU_CSV):
        print(f"‚ùå –õ–∏–ø—Å–≤–∞ {SKU_CSV}")
        sys.exit(1)

    # –í–ò–ù–ê–ì–ò –∑–∞–Ω—É–ª—è–≤–∞–º–µ —Ä–µ–∑—É–ª—Ç–∞—Ç–∏—Ç–µ –Ω–∞ –≤—Å—è–∫–æ –ø—É—Å–∫–∞–Ω–µ
    init_result_files()

    all_skus = read_skus(SKU_CSV)
    print(f"üßæ –û–±—â–æ SKU –≤ CSV: {len(all_skus)}")

    count = 0
    for sku in all_skus:
        process_one_sku(sku)
        count += 1
        time.sleep(DELAY_BETWEEN_SKUS)  # —â–∞–¥—è—â–∞ –ø–∞—É–∑–∞ –º–µ–∂–¥—É SKU
        if count % 100 == 0:
            print(f"üì¶ –ü—Ä–æ–≥—Ä–µ—Å: {count}/{len(all_skus)} –≥–æ—Ç–æ–≤–∏")

    print(f"\n‚úÖ –†–µ–∑—É–ª—Ç–∞—Ç–∏: {RES_CSV}")
    print(f"üìÑ Not found: {NF_CSV}")

if __name__ == "__main__":
    main()
