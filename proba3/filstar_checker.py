#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Filstar checker ‚Äî –°–ï–†–ò–ô–ù–û (–±–µ–∑ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ—Å—Ç, –±–µ–∑ –Ω–∏—à–∫–∏)
- –ó–∞ –≤—Å–µ–∫–∏ SKU: /search?term=<sku> -> –∫–∞–Ω–¥–∏–¥–∞—Ç–∏ -> –ø—Ä–æ–¥—É–∫—Ç -> —Ä–µ–¥ "–ö–û–î" -> —Ü–µ–Ω–∞/–±—Ä–æ–π–∫–∞
- –©–∞–¥—è—â–æ –∫—ä–º —Å–∞–π—Ç–∞: –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞—è–≤–∫–∏ –∏ –º–µ–∂–¥—É SKU
- Resume: –ø—Ä–µ—Å–∫–∞—á–∞ –≤–µ—á–µ –æ–±—Ä–∞–±–æ—Ç–µ–Ω–∏ (results/not_found/processed.txt)
- Debug: –∑–∞–ø–∏—Å–≤–∞ HTML –ø—Ä–∏ –ø—Ä–æ–±–ª–µ–º
"""

import csv
import os
import re
import sys
import time
from typing import List, Tuple, Optional
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup

# ---------------- –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è ----------------
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
SKU_CSV = os.path.join(BASE_DIR, "sku_list_filstar.csv")
RES_CSV = os.path.join(BASE_DIR, "results_filstar.csv")
NF_CSV  = os.path.join(BASE_DIR, "not_found_filstar.csv")
STATE_FILE = os.path.join(BASE_DIR, "processed.txt")

DEBUG_DIR = os.path.join(BASE_DIR, "debug_html")
os.makedirs(DEBUG_DIR, exist_ok=True)

SEARCH_URLS = [
    "https://filstar.com/search?term={q}",
    "https://filstar.com/bg/search?term={q}",
    "https://filstar.com/en/search?term={q}",
]

# –©–∞–¥—è—â–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ (–º–æ–∂–µ—à –¥–∞ —É–≤–µ–ª–∏—á–∏—à –ø–∞—É–∑–∏—Ç–µ –ø—Ä–∏ –Ω—É–∂–¥–∞)
REQUEST_DELAY = 0.4       # —Å–µ–∫. –ø–∞—É–∑–∞ –º–µ–∂–¥—É HTTP –∑–∞—è–≤–∫–∏
DELAY_BETWEEN_SKUS = 0.8  # —Å–µ–∫. –ø–∞—É–∑–∞ –º–µ–∂–¥—É SKU
TIMEOUT = 20              # —Å–µ–∫. —Ç–∞–π–º–∞—É—Ç –Ω–∞ –∑–∞—è–≤–∫–∞
RETRIES = 3               # —Ä–µ—Ç—Ä–∞–∏–∏ –Ω–∞ –∑–∞—è–≤–∫–∞
MAX_CANDIDATES = 15       # –¥–æ –∫–æ–ª–∫–æ –ø—Ä–æ–¥—É–∫—Ç–æ–≤–∏ –ª–∏–Ω–∫–∞ –¥–∞ –ø—Ä–æ–≤–µ—Ä–∏–º –æ—Ç —Ç—ä—Ä—Å–∞—á–∫–∞—Ç–∞

HEADERS = {
    "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
                  "(KHTML, like Gecko) Chrome/127 Safari/537.36",
    "Accept-Language": "bg-BG,bg;q=0.9,en-US;q=0.8,en;q=0.7",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Connection": "keep-alive",
}

# ---------------- –ü–æ–º–æ—â–Ω–∏ ----------------
def norm(s: str) -> str:
    return (s or "").strip()

def only_digits(s: str) -> str:
    return re.sub(r"\D+", "", s or "")

def save_debug_html_text(text: str, sku: str, tag: str):
    try:
        path = os.path.join(DEBUG_DIR, f"debug_{sku}_{tag}.html")
        with open(path, "w", encoding="utf-8") as f:
            f.write(text)
        print(f"   üêû Debug HTML –∑–∞–ø–∏—Å–∞–Ω: {path}")
    except Exception:
        pass

# ---------------- HTTP –∫–ª–∏–µ–Ω—Ç (—Å–µ—Ä–∏–π–Ω–æ, —Å –ø–∞—É–∑–∞) ----------------
class Http:
    def __init__(self):
        self.s = requests.Session()
        self.s.headers.update(HEADERS)
        self._last_ts = 0.0

    def get(self, url: str) -> requests.Response:
        # —â–∞–¥—è—â–∞ –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞—è–≤–∫–∏
        elapsed = time.time() - self._last_ts
        if elapsed < REQUEST_DELAY:
            time.sleep(REQUEST_DELAY - elapsed)
        for attempt in range(1, RETRIES + 1):
            try:
                r = self.s.get(url, timeout=TIMEOUT, allow_redirects=True)
                self._last_ts = time.time()
                return r
            except requests.RequestException as e:
                if attempt == RETRIES:
                    raise
                time.sleep(0.6 * attempt)

HTTP = Http()

# ---------------- I/O ----------------
def read_skus(path: str) -> List[str]:
    skus = []
    with open(path, newline="", encoding="utf-8") as f:
        r = csv.reader(f)
        _ = next(r, None)  # —Ö–µ–¥—ä—Ä
        for row in r:
            if not row: continue
            v = norm(row[0])
            if v and v.lower() != "sku":
                skus.append(v)
    return skus

def ensure_result_headers():
    if not os.path.exists(RES_CSV):
        with open(RES_CSV, "w", newline="", encoding="utf-8") as f:
            w = csv.writer(f)
            w.writerow(["SKU", "–ù–∞–ª–∏—á–Ω–æ—Å—Ç", "–ë—Ä–æ–π–∫–∏", "–¶–µ–Ω–∞ (–Ω–æ—Ä–º–∞–ª–Ω–∞ –ª–≤.)"])
    if not os.path.exists(NF_CSV):
        with open(NF_CSV, "w", newline="", encoding="utf-8") as f:
            w = csv.writer(f)
            w.writerow(["SKU"])

def append_result(row):
    with open(RES_CSV, "a", newline="", encoding="utf-8") as f:
        csv.writer(f).writerow(row)

def append_nf(sku: str):
    with open(NF_CSV, "a", newline="", encoding="utf-8") as f:
        csv.writer(f).writerow([sku])

def load_done_sets() -> set:
    done = set()
    if os.path.exists(RES_CSV):
        with open(RES_CSV, newline="", encoding="utf-8") as f:
            r = csv.reader(f); _ = next(r, None)
            for row in r:
                if row: done.add(norm(row[0]))
    if os.path.exists(NF_CSV):
        with open(NF_CSV, newline="", encoding="utf-8") as f:
            r = csv.reader(f); _ = next(r, None)
            for row in r:
                if row: done.add(norm(row[0]))
    if os.path.exists(STATE_FILE):
        with open(STATE_FILE, encoding="utf-8") as f:
            for line in f:
                if norm(line): done.add(norm(line))
    return done

def append_state(sku: str):
    with open(STATE_FILE, "a", encoding="utf-8") as f:
        f.write(sku + "\n")

# ---------------- –ü–∞—Ä—Å–µ–Ω–µ ----------------
def parse_search_candidates(html: str) -> List[str]:
    soup = BeautifulSoup(html, "lxml")
    out = []
    for a in soup.select(".product-item-wapper a.product-name"):
        href = (a.get("href") or "").strip()
        if not href:
            continue
        if href.startswith("/"):
            href = urljoin("https://filstar.com", href)
        out.append(href)
    # –ø—Ä–µ–º–∞—Ö–Ω–∏ –¥—É–±–ª–∏ –∏ —Ä–µ–∂–∏ –¥–æ MAX_CANDIDATES
    seen, uniq = set(), []
    for h in out:
        if h not in seen:
            seen.add(h)
            uniq.append(h)
    return uniq[:MAX_CANDIDATES]

def extract_row_data_from_product_html(html: str, sku: str) -> Tuple[Optional[str], Optional[int], Optional[str]]:
    """
    –í—Ä—ä—â–∞ (status, qty, normal_price_lv) –∑–∞ —Ç–æ—á–Ω–∏—è —Ä–µ–¥ –ø–æ '–ö–û–î'.
    - –ù–æ—Ä–º–∞–ª–Ω–∞ —Ü–µ–Ω–∞: –æ—Ç <strike> –∞–∫–æ –∏–º–∞, –∏–Ω–∞—á–µ –ø—ä—Ä–≤–∞—Ç–∞ '... –ª–≤.' –≤ —Ü–µ–Ω–æ–≤–∞—Ç–∞ –∫–ª–µ—Ç–∫–∞/—Ä–µ–¥–∞.
    - –ë—Ä–æ–π–∫–∞: –æ—Ç counter-box input[type=text] value (–≤–∏–¥–∏–º–∞ –∏ –±–µ–∑ –ª–æ–≥–∏–Ω).
    """
    soup = BeautifulSoup(html, "lxml")
    tbody = soup.select_one("#fast-order-table tbody")
    if not tbody:
        return None, None, None

    rows = tbody.select("tr")
    for row in rows:
        code_td = row.select_one("td.td-sky")
        code_digits = only_digits(code_td.get_text(" ", strip=True)) if code_td else ""
        if code_digits != str(sku):
            continue

        # –¶–µ–Ω–∞ (–Ω–æ—Ä–º–∞–ª–Ω–∞)
        price = None
        strike = row.find("strike")
        if strike:
            m = re.search(r"(\d+[.,]?\d*)\s*–ª–≤", strike.get_text(" ", strip=True), re.I)
            if m:
                price = m.group(1).replace(",", ".")
        if not price:
            price_td = None
            for td in row.find_all("td"):
                if td.find(string=lambda t: isinstance(t, str) and "–¶–ï–ù–ê –ù–ê –î–†–ï–ë–ù–û" in t):
                    price_td = td
                    break
            txt = price_td.get_text(" ", strip=True) if price_td else row.get_text(" ", strip=True)
            m2 = re.search(r"(\d+[.,]?\d*)\s*–ª–≤", txt, re.I)
            if m2:
                price = m2.group(1).replace(",", ".")

        # –ù–∞–ª–∏—á–Ω–æ—Å—Ç (counter-box)
        qty = 0
        status = "Unknown"
        inp = row.select_one(".counter-box input[type='text']")
        if inp:
            val = (inp.get("value") or "").strip()
            if val.isdigit():
                qty = int(val)
                status = "–ù–∞–ª–∏—á–µ–Ω" if qty > 0 else "–ò–∑—á–µ—Ä–ø–∞–Ω"

        return status, qty, price

    return None, None, None

# ---------------- –õ–æ–≥–∏–∫–∞ –∑–∞ 1 SKU (—Å–µ—Ä–∏–π–Ω–æ) ----------------
def process_one_sku(sku: str):
    q = only_digits(sku) or sku
    print(f"\n‚û°Ô∏è –û–±—Ä–∞–±–æ—Ç–≤–∞–º SKU: {q}")

    # 1) —Ç—ä—Ä—Å–µ–Ω–µ
    candidates = []
    for su in SEARCH_URLS:
        url = su.format(q=q)
        try:
            r = HTTP.get(url)
            if r.status_code == 200 and r.text:
                c = parse_search_candidates(r.text)
                if c:
                    candidates.extend(c)
        except Exception:
            pass
        if len(candidates) >= MAX_CANDIDATES:
            break

    # —É–Ω–∏–∫–∞–ª–∏–∑–∏—Ä–∞–π
    seen, uniq = set(), []
    for h in candidates:
        if h not in seen:
            seen.add(h)
            uniq.append(h)
    candidates = uniq[:MAX_CANDIDATES]

    if not candidates:
        print(f"‚ùå –ù—è–º–∞ —Ä–µ–∑—É–ª—Ç–∞—Ç–∏ –≤ —Ç—ä—Ä—Å–∞—á–∫–∞—Ç–∞ –∑–∞ {q}")
        append_nf(q)
        append_state(q)
        return

    # 2) –æ–±—Ö–æ–∂–¥–∞–º–µ –∫–∞–Ω–¥–∏–¥–∞—Ç –ø—Ä–æ–¥—É–∫—Ç–∏—Ç–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–Ω–æ
    for link in candidates:
        try:
            r = HTTP.get(link)
            if r.status_code != 200 or not r.text:
                continue
            status, qty, price = extract_row_data_from_product_html(r.text, q)
            if price is not None:
                print(f"  ‚úÖ {q} ‚Üí {price} –ª–≤. | {status} ({qty} –±—Ä.) | {link}")
                append_result([q, status or "Unknown", qty or 0, price])
                append_state(q)
                return
            else:
                # –∑–∞–ø–∞–∑–∏ –¥–µ–±—ä–≥ HTML —Å–∞–º–æ –≤–µ–¥–Ω—ä–∂ (–æ—Ç –ø—ä—Ä–≤–∏—è –∫–∞–Ω–¥–∏–¥–∞—Ç)
                save_debug_html_text(r.text, q, "no_price_or_row")
        except Exception:
            continue

    print(f"‚ùå –ù–µ –Ω–∞–º–µ—Ä–∏—Ö SKU {q} –≤ {len(candidates)} —Ä–µ–∑—É–ª—Ç–∞—Ç–∞.")
    append_nf(q)
    append_state(q)

# ---------------- main (—Å–µ—Ä–∏–π–Ω–æ) ----------------
def main():
    if not os.path.exists(SKU_CSV):
        print(f"‚ùå –õ–∏–ø—Å–≤–∞ {SKU_CSV}")
        sys.exit(1)

    ensure_result_headers()

    all_skus = read_skus(SKU_CSV)
    print(f"üßæ –û–±—â–æ SKU –≤ CSV: {len(all_skus)}")

    already = load_done_sets()
    todo = [s for s in all_skus if norm(s) not in already]

    print(f"‚è© –ü—Ä–µ—Å–∫–∞—á–∞–º –≤–µ—á–µ –æ–±—Ä–∞–±–æ—Ç–µ–Ω–∏: {len(already)}")
    print(f"üö∂ –°–µ—Ä–∏–π–Ω–æ –∑–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–µ–≥–∞: {len(todo)}")

    count = 0
    for sku in todo:
        process_one_sku(sku)
        count += 1
        # —â–∞–¥—è—â–∞ –ø–∞—É–∑–∞ –º–µ–∂–¥—É –æ—Ç–¥–µ–ª–Ω–∏—Ç–µ SKU
        time.sleep(DELAY_BETWEEN_SKUS)
        if count % 100 == 0:
            print(f"üì¶ –ü—Ä–æ–≥—Ä–µ—Å: {count}/{len(todo)} –≥–æ—Ç–æ–≤–∏")

    print(f"\n‚úÖ –†–µ–∑—É–ª—Ç–∞—Ç–∏: {RES_CSV}")
    print(f"üìÑ Not found: {NF_CSV}")
    print(f"üß∑ State: {STATE_FILE}")

if __name__ == "__main__":
    main()
