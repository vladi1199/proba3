import csv
import os
import re
import time
from urllib.parse import urljoin

import requests
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC


# ---------------- –ö–æ–Ω—Å—Ç–∞–Ω—Ç–∏ –∏ –ø—ä—Ç–∏—â–∞ ----------------
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
SKU_CSV = os.path.join(BASE_DIR, "sku_list_filstar.csv")
RES_CSV = os.path.join(BASE_DIR, "results_filstar.csv")
NF_CSV  = os.path.join(BASE_DIR, "not_found_filstar.csv")

HTTP_HEADERS = {
    "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
                  "(KHTML, like Gecko) Chrome/127 Safari/537.36",
    "Accept-Language": "bg-BG,bg;q=0.9,en-US;q=0.8,en;q=0.7",
}


# ---------------- WebDriver ----------------
def create_driver():
    opts = Options()
    opts.add_argument("--headless=new")
    opts.add_argument("--no-sandbox")
    opts.add_argument("--disable-dev-shm-usage")
    opts.add_argument("--window-size=1440,1400")
    opts.add_argument("--lang=bg-BG,bg,en-US,en")
    opts.add_experimental_option("excludeSwitches", ["enable-automation"])
    opts.add_experimental_option("useAutomationExtension", False)

    driver = webdriver.Chrome(options=opts)
    driver.set_page_load_timeout(40)
    return driver


def click_cookies_if_any(driver):
    candidates = [
        (By.CSS_SELECTOR, "#onetrust-accept-btn-handler"),
        (By.XPATH, "//button[contains(.,'–ü—Ä–∏–µ–º–∞–º')]"),
        (By.XPATH, "//*[contains(.,'–ü—Ä–∏–µ–º–∞–º –±–∏—Å–∫–≤–∏—Ç–∫–∏—Ç–µ')]"),
        (By.XPATH, "//button[contains(.,'Accept')]"),
    ]
    for how, sel in candidates:
        try:
            WebDriverWait(driver, 3).until(EC.element_to_be_clickable((how, sel))).click()
            break
        except Exception:
            pass


# ---------------- –£—Ç–∏–ª–∏—Ç–∏ ----------------
def norm(s): return str(s).strip()

def save_debug_html_text(text, sku, tag):
    try:
        path = os.path.join(BASE_DIR, f"debug_{sku}_{tag}.html")
        with open(path, "w", encoding="utf-8") as f:
            f.write(text)
        print(f"   üêû –ó–∞–ø–∏—Å–∞—Ö HTML –∑–∞ {sku}: {path}")
    except Exception:
        pass

def save_debug_html(driver, sku, tag):
    try:
        save_debug_html_text(driver.page_source, sku, tag)
    except Exception:
        pass


# ---------------- –¢—ä—Ä—Å–µ–Ω–µ –Ω–∞ –ø—Ä–æ–¥—É–∫—Ç ----------------
def find_product_link_via_search(driver, sku) -> str | None:
    """–û—Ç–≤–∞—Ä—è /search?term=<SKU> –∏ –≤–∑–∏–º–∞ –ø—ä—Ä–≤–∏—è —Ä–µ–∞–ª–µ–Ω –ø—Ä–æ–¥—É–∫—Ç –æ—Ç .product-item-wapper a.product-name"""
    q = norm(sku)
    search_urls = [
        f"https://filstar.com/search?term={q}",
        f"https://filstar.com/bg/search?term={q}",
        f"https://filstar.com/en/search?term={q}",
    ]
    for surl in search_urls:
        try:
            driver.get(surl)
            click_cookies_if_any(driver)
            WebDriverWait(driver, 14).until(
                EC.presence_of_all_elements_located((By.CSS_SELECTOR, ".product-item-wapper a.product-name"))
            )
            anchors = driver.find_elements(By.CSS_SELECTOR, ".product-item-wapper a.product-name")
            for a in anchors:
                href = (a.get_attribute("href") or "").strip()
                if not href:
                    continue
                if href.startswith("/"):
                    href = urljoin("https://filstar.com", href)
                return href
        except Exception:
            continue
    return None


# ---------------- –ò–∑–≤–ª–∏—á–∞–Ω–µ –ø—Ä–µ–∑ Selenium ----------------
def scroll_until_row_with_sku(driver, sku, max_steps=24, step_px=900, pause=0.35):
    q = norm(sku)
    xp = f"//table[@id='fast-order-table']//tr[td[contains(@class,'td-sky')][contains(normalize-space(),'{q}')]]"
    for _ in range(max_steps):
        try:
            row = driver.find_elements(By.XPATH, xp)
            if row:
                return True
        except Exception:
            pass
        try:
            driver.execute_script(f"window.scrollBy(0, {step_px});")
            driver.execute_script("window.dispatchEvent(new Event('scroll'));")
        except Exception:
            pass
        time.sleep(pause)
    # —Ñ–∏–Ω–∞–ª–Ω–æ ‚Äì –¥–æ –¥—ä–Ω–æ—Ç–æ
    try:
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        driver.execute_script("window.dispatchEvent(new Event('scroll'));")
    except Exception:
        pass
    time.sleep(pause + 0.4)
    try:
        row = driver.find_elements(By.XPATH, xp)
        return bool(row)
    except Exception:
        return False


def find_row_by_sku_in_table(driver, sku):
    q = norm(sku)
    try:
        tbody = WebDriverWait(driver, 12).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "#fast-order-table tbody"))
        )
    except Exception:
        return None, None

    rows = tbody.find_elements(By.CSS_SELECTOR, "tr")
    print(f"   üîé DEBUG: –Ω–∞–º–µ—Ä–µ–Ω–∏ —Ä–µ–¥–æ–≤–µ –≤ —Ç–∞–±–ª–∏—Ü–∞—Ç–∞: {len(rows)}")

    for idx, row in enumerate(rows, start=1):
        try:
            code_td = row.find_element(By.CSS_SELECTOR, "td.td-sky")
            code_text = (code_td.text or "").replace("\xa0", " ").strip()
            code_digits = re.sub(r"\D+", "", code_text)
            print(f"      ‚Ä¢ —Ä–µ–¥ {idx}: code_cell='{code_text}' (digits='{code_digits}')")
            if code_digits == q:
                print(f"      ‚úÖ —Å—ä–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ SKU –≤ —Ä–µ–¥ {idx}")
                return row, code_text
        except Exception:
            continue

    return None, None


def extract_price_from_row_via_selenium(row_el):
    # 1) –Ω–æ—Ä–º–∞–ª–Ω–∞—Ç–∞ (—Å—Ç–∞—Ä–∞) —Ü–µ–Ω–∞ –≤ <strike>
    try:
        for st in row_el.find_elements(By.TAG_NAME, "strike"):
            raw = (st.text or "").replace("\xa0", " ")
            m = re.search(r"(\d+[.,]?\d*)\s*–ª–≤", raw, flags=re.IGNORECASE)
            if m:
                return m.group(1).replace(",", ".")
    except Exception:
        pass

    # 2) —Ü–µ–Ω–æ–≤–∞ –∫–ª–µ—Ç–∫–∞ ‚Äû–¶–ï–ù–ê –ù–ê –î–†–ï–ë–ù–û‚Äú (–≤—ä—Ç—Ä–µ—à–µ–Ω —Ç–µ–∫—Å—Ç)
    try:
        price_td = row_el.find_element(By.XPATH, ".//td[.//span[contains(.,'–¶–ï–ù–ê –ù–ê –î–†–ï–ë–ù–û')]]")
        txt = (price_td.text or "").replace("\xa0", " ")
        m = re.search(r"(\d+[.,]?\d*)\s*–ª–≤", txt, flags=re.IGNORECASE)
        if m:
            return m.group(1).replace(",", ".")
        # 2b) –∞–∫–æ —Ç–µ–∫—Å—Ç—ä—Ç –µ –ø—Ä–∞–∑–µ–Ω ‚Äì –ø—Ä–æ–±–≤–∞–π innerHTML
        html = price_td.get_attribute("innerHTML") or ""
        m2 = re.search(r"(\d+[.,]?\d*)\s*(?:&nbsp;)?–ª–≤", html, flags=re.IGNORECASE)
        if m2:
            return m2.group(1).replace(",", ".")
    except Exception:
        pass

    # 3) fallback: –ø—ä—Ä–≤–∞—Ç–∞ ‚Äû‚Ä¶ –ª–≤.‚Äú –≤ —Ü–µ–ª–∏—è —Ä–µ–¥ (text –∏–ª–∏ innerHTML)
    try:
        txt = (row_el.text or "").replace("\xa0", " ")
        m = re.search(r"(\d+[.,]?\d*)\s*–ª–≤", txt, flags=re.IGNORECASE)
        if m:
            return m.group(1).replace(",", ".")
        html = row_el.get_attribute("innerHTML") or ""
        m2 = re.search(r"(\d+[.,]?\d*)\s*(?:&nbsp;)?–ª–≤", html, flags=re.IGNORECASE)
        if m2:
            return m2.group(1).replace(",", ".")
    except Exception:
        pass

    return None


# ---------------- Fallback: –∏–∑–≤–ª–∏—á–∞–Ω–µ –ø—Ä–µ–∑ requests (–±–µ–∑ JS) ----------------
def extract_price_via_requests(product_url, sku):
    """–ü–∞—Ä—Å–≤–∞ —Å—É—Ä–æ–≤–∏—è HTML –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ç–∞ –∏ –Ω–∞–º–∏—Ä–∞ —Ü–µ–Ω–∞ –∑–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏—è SKU."""
    try:
        r = requests.get(product_url, headers=HTTP_HEADERS, timeout=20)
        r.raise_for_status()
        html = r.text
        # –∑–∞–ø–∞–∑–∏ –¥–µ–±—ä–≥ –ø—Ä–∏ –Ω—É–∂–¥–∞
        # save_debug_html_text(html, sku, "requests_page")

        # –∏–∑–æ–ª–∏—Ä–∞–π tbody –Ω–∞ —Ç–∞–±–ª–∏—Ü–∞—Ç–∞
        m_tbody = re.search(
            r'<table[^>]*id=["\']fast-order-table["\'][^>]*>.*?<tbody>(.*?)</tbody>',
            html, re.S | re.I
        )
        if not m_tbody:
            return None
        tbody = m_tbody.group(1)

        # –≤—Å–∏—á–∫–∏ —Ä–µ–¥–æ–≤–µ
        rows = re.findall(r'<tr[^>]*class=["\']table-row-scroll["\'][^>]*>(.*?)</tr>',
                          tbody, re.S | re.I)
        for row_html in rows:
            # –∫–æ–ª–æ–Ω–∞ –ö–û–î
            m_code = re.search(r'class=["\'][^"\']*td-sky[^"\']*["\'][^>]*>(.*?)</td>',
                               row_html, re.S | re.I)
            if not m_code:
                continue
            code_text = re.sub(r"<[^>]*>", "", m_code.group(1))
            code_digits = re.sub(r"\D+", "", code_text)
            if code_digits != str(sku):
                continue

            # —Ü–µ–Ω–∞ (–Ω–æ—Ä–º–∞–ª–Ω–∞): –ø—ä—Ä–≤–æ strike, –∏–Ω–∞—á–µ –ø—ä—Ä–≤–∞—Ç–∞ '... –ª–≤'
            m_strike = re.search(r"<strike[^>]*>(.*?)</strike>", row_html, re.S | re.I)
            if m_strike:
                s_txt = re.sub(r"<[^>]*>", "", m_strike.group(1))
                m_p = re.search(r"(\d+[.,]?\d*)\s*–ª–≤", s_txt, re.I)
                if m_p:
                    return m_p.group(1).replace(",", ".")

            m_any = re.search(r"(\d+[.,]?\d*)\s*(?:&nbsp;)?–ª–≤", row_html, re.I)
            if m_any:
                return m_any.group(1).replace(",", ".")

        return None
    except Exception:
        return None


# ---------------- –ö–æ–º–±–∏–Ω–∏—Ä–∞–Ω–æ –∏–∑–≤–ª–∏—á–∞–Ω–µ –æ—Ç –ø—Ä–æ–¥—É–∫—Ç–æ–≤–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ ----------------
def scrape_product_page(driver, product_url, sku):
    driver.get(product_url)
    click_cookies_if_any(driver)

    # –∏–∑—á–∞–∫–∞–π —Ç–∞–±–ª–∏—Ü–∞—Ç–∞ (–∞–∫–æ –∏–º–∞)
    try:
        WebDriverWait(driver, 12).until(EC.presence_of_element_located((By.ID, "fast-order-table")))
    except Exception:
        pass

    # —Å–∫—Ä–æ–ª (lazy-load)
    scroll_until_row_with_sku(driver, sku)

    print(f"   üîé TITLE: {driver.title.strip()[:120]}")
    print(f"   üîé URL:   {driver.current_url}")

    # –æ–ø–∏—Ç 1: Selenium
    row, _ = find_row_by_sku_in_table(driver, sku)
    price = None
    if row:
        price = extract_price_from_row_via_selenium(row)

    # –æ–ø–∏—Ç 2 (fallback): requests, –∞–∫–æ Selenium –Ω–µ –Ω–∞–º–µ—Ä–∏ —Ä–µ–¥/—Ü–µ–Ω–∞
    if price is None:
        print("   üîÅ Fallback: –∏–∑–≤–ª–∏—á–∞–º –ø—Ä–µ–∑ requests()‚Ä¶")
        price = extract_price_via_requests(product_url, sku)
        if price is None:
            save_debug_html(driver, sku, tag="no_price_or_row")

    status, qty = "Unknown", 0
    return status, qty, price


# ---------------- CSV I/O ----------------
def read_sku_codes(path):
    skus = []
    with open(path, newline="", encoding="utf-8") as f:
        r = csv.reader(f)
        header = next(r, None)  # –ø—Ä–æ–ø—É—Å–∫–∞–º–µ —Ö–µ–¥—ä—Ä–∞
        for row in r:
            if not row:
                continue
            val = (row[0] or "").strip()
            if not val or val.lower() == "sku":
                continue
            skus.append(val)
    if skus:
        print(f"   üßæ SKUs loaded ({len(skus)}): {', '.join(skus[:5])}{' ...' if len(skus)>5 else ''}")
    else:
        print("   üßæ No SKUs loaded from CSV.")
    return skus


def save_results(rows, path):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        w.writerow(["SKU", "–ù–∞–ª–∏—á–Ω–æ—Å—Ç", "–ë—Ä–æ–π–∫–∏", "–¶–µ–Ω–∞ (–Ω–æ—Ä–º–∞–ª–Ω–∞ –ª–≤.)"])
        w.writerows(rows)


def save_not_found(skus, path):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        w.writerow(["SKU"])
        for s in skus:
            w.writerow([s])


# ---------------- main ----------------
def main():
    skus = read_sku_codes(SKU_CSV)
    driver = create_driver()
    results, not_found = [], []

    try:
        for sku in skus:
            print(f"\n‚û°Ô∏è –û–±—Ä–∞–±–æ—Ç–≤–∞–º SKU: {sku}")

            product_url = find_product_link_via_search(driver, sku)
            if not product_url:
                print(f"‚ùå –ù–µ –Ω–∞–º–µ—Ä–∏—Ö –ø—Ä–æ–¥—É–∫—Ç –∑–∞ {sku} –≤ —Ç—ä—Ä—Å–∞—á–∫–∞—Ç–∞.")
                not_found.append(sku)
                continue

            print(f"  ‚úÖ –ü—Ä–æ–¥—É–∫—Ç –ª–∏–Ω–∫: {product_url}")
            status, qty, price = scrape_product_page(driver, product_url, sku)
            print(f"     ‚Üí –°—Ç–∞—Ç—É—Å: {status} | –ë—Ä–æ–π–∫–∏: {qty} | –¶–µ–Ω–∞: {price if price else '‚Äî'}")

            results.append([sku, status, qty, price or ""])
            time.sleep(0.2)

    finally:
        driver.quit()

    save_results(results, RES_CSV)
    save_not_found(not_found, NF_CSV)
    print(f"\n‚úÖ –ó–∞–ø–∞–∑–µ–Ω–∏ —Ä–µ–∑—É–ª—Ç–∞—Ç–∏: {RES_CSV}")
    print(f"‚ùå –ù–µ–Ω–∞–º–µ—Ä–µ–Ω–∏ SKU –∫–æ–¥–æ–≤–µ: {NF_CSV}")


if __name__ == "__main__":
    main()
